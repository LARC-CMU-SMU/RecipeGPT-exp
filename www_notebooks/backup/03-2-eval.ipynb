{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "# typically takes 30 secs\n",
    "from nlp_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "time: 28.3 ms\n"
     ]
    }
   ],
   "source": [
    "database = load_pickle('../big_data/database2.pickle')\n",
    "database = list(set([word for word in database if word!= '-PRON-']))\n",
    "print(len(database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.9 ms\n"
     ]
    }
   ],
   "source": [
    "class ev(evaluation):\n",
    "    def __init__(self, filename, tag):\n",
    "        self.dic = self.load_dic({}, filename, tag)\n",
    "        self.ori = tag\n",
    "        self.gens = []\n",
    "\n",
    "    def ingr_f1(self, root = True):\n",
    "        value, number = [], []\n",
    "        for i, v in tqdm.tqdm(self.dic.items()):\n",
    "            true, pred = v['%s_ingr'%(self.ori)], v['%s_ingr'%(self.gen)]\n",
    "            if root:\n",
    "                true, pred = self.ingr(true), self.ingr(pred)\n",
    "            scores = metrics(true, pred)\n",
    "            value.append(scores.f1())\n",
    "            number.append(len(set(pred)))\n",
    "        avg = sum(value)/len(value)\n",
    "        print(avg)\n",
    "        print('average ingr number', sum(number)/len(number))\n",
    "        return avg\n",
    "    \n",
    "    def ingr_precision_recall(self, generate = 'ingr'):\n",
    "        assert generate in ['ingr','instr','human']\n",
    "        precision, recall = [], []\n",
    "        for i, v in tqdm.tqdm(self.dic.items()):\n",
    "            if generate == 'ingr':\n",
    "                true, pred = v['%s_ingr'%(self.ori)], v['%s_ingr'%(self.gen)]\n",
    "                true, pred = self.ingr(true), self.ingr(pred)\n",
    "            elif generate == 'instr':\n",
    "                true, pred = v['%s_ingr'%(self.ori)], v['%s_instr'%(self.gen)]\n",
    "                true, pred = self.ingr(true), self.instr(pred)\n",
    "            elif generate == 'human':   \n",
    "                true, pred = v['%s_ingr'%(self.ori)], v['%s_instr'%(self.ori)]\n",
    "                true, pred = self.ingr(true), self.instr(pred)\n",
    "            scores = metrics(true, pred)\n",
    "            precision.append(scores.precision())\n",
    "            recall.append(scores.recall())\n",
    "    \n",
    "        print('precision', sum(precision)/len(precision))\n",
    "        print('recall', sum(recall)/len(recall))\n",
    "    \n",
    "    def instr(self, directions):\n",
    "        instr = sp.spacy(directions)\n",
    "        root_instr = []\n",
    "        for chunk in instr.noun_chunks:\n",
    "            idx_rootnoun = chunk.end - 1\n",
    "            str_rootnoun = instr[idx_rootnoun].lemma_\n",
    "            if str_rootnoun in database:\n",
    "                root_instr.append(str_rootnoun)\n",
    "        return root_instr\n",
    "    \n",
    "    def ingr(self, lst):\n",
    "        '''\n",
    "        Args: lst: a list of ingredient names\n",
    "        used when len(lst) must equal to root_match\n",
    "        '''\n",
    "        hl = [[{'text':x, 'highlight': None} for x in i.split(' ')] for i in lst]\n",
    "        root_match = []\n",
    "        for i, ingr in enumerate(lst):\n",
    "            if ' ' not in ingr:\n",
    "                hl[i][0]['highlight'] = 'wrong'\n",
    "                doc = sp.spacy(ingr)\n",
    "                root_match.append(doc[0].lemma_)\n",
    "            else:\n",
    "                phrase = 'Mix the %s and water.'%ingr\n",
    "                doc = sp.spacy(phrase)\n",
    "                \n",
    "                last_chunk = None\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    if chunk.text != 'water':\n",
    "                        last_chunk = chunk\n",
    "                if not last_chunk:\n",
    "                    root_match.append('CANNOT_DETECT')\n",
    "                else:\n",
    "                    found = False\n",
    "                    for j, word in enumerate(hl[i]):\n",
    "                        if doc[last_chunk.end - 1].text in word['text']:\n",
    "                            hl[i][j]['highlight'] = 'wrong' \n",
    "                            root_match.append(doc[last_chunk.end - 1].lemma_)\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        root_match.append('CANNOT_DETECT')\n",
    "                        \n",
    "        assert len(root_match) == len(lst)\n",
    "        return root_match\n",
    "\n",
    "    '''\n",
    "    exporting data\n",
    "    '''\n",
    "    def to_bleu(self):\n",
    "        to_write = {'%s_i'%(self.ori):'',\n",
    "                    '%s_i'%(self.gen):'',\n",
    "                    '%s_d'%(self.ori):'',\n",
    "                    '%s_d'%(self.gen):''}\n",
    "        \n",
    "        for i, v in self.dic.items():\n",
    "            to_write['%s_i'%(self.ori)] += self.add_space(' $ '.join(v['%s_ingr'%(self.ori)]))+ ' $ \\n'\n",
    "            to_write['%s_i'%(self.gen)] += self.add_space(' $ '.join(v['%s_ingr'%(self.gen)])) + ' $ \\n'\n",
    "            \n",
    "            to_write['%s_d'%(self.ori)] += self.add_space(v['%s_instr'%(self.ori)])+ '\\n'\n",
    "            to_write['%s_d'%(self.gen)] += self.add_space(v['%s_instr'%(self.gen)])+ '\\n'\n",
    "        \n",
    "        for k, v in to_write.items():\n",
    "            save('../../to_gpt2/generation_%s.txt'%(k), v ,overwrite = True)\n",
    "        !eval {\"perl multi-bleu.perl ../../to_gpt2/generation_%s_i.txt < ../../to_gpt2/generation_%s_i.txt\" %(self.ori, self.gen)}\n",
    "        !eval {\"perl multi-bleu.perl ../../to_gpt2/generation_%s_d.txt < ../../to_gpt2/generation_%s_d.txt\" %(self.ori, self.gen)}\n",
    "    \n",
    "        !eval {\"rouge -f ../../to_gpt2/generation_%s_i.txt ../../to_gpt2/generation_%s_i.txt --avg\"%(self.ori, self.gen)}\n",
    "        !eval {\"rouge -f ../../to_gpt2/generation_%s_d.txt ../../to_gpt2/generation_%s_d.txt --avg\"%(self.ori, self.gen)}\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../../to_gpt2/recipe1M_1218/test/y/\n",
      "load ../../to_gpt2/generation_1220_k3_test/\n",
      "load ../../to_gpt2/generation_scratch_k3_test/\n",
      "load ../../to_gpt2/generation_medium_k3_test/\n",
      "time: 3.23 s\n"
     ]
    }
   ],
   "source": [
    "data = ev('../../to_gpt2/recipe1M_1218/test/y/', 'ori')\n",
    "data.append_dic('../../to_gpt2/generation_1220_k3_test/', '117M')\n",
    "data.append_dic('../../to_gpt2/generation_scratch_k3_test/', 'scratch')\n",
    "data.append_dic('../../to_gpt2/generation_medium_k3_test/', '345M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117M\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_117M_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_117M_d.txt\n",
      "BLEU = 32.82, 83.6/63.9/33.7/13.0 (BP=0.839, ratio=0.850, hyp_len=84162, ref_len=98982)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 8.34, 53.6/18.5/6.8/2.8 (BP=0.708, ratio=0.743, hyp_len=381274, ref_len=513062)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.7119716334533762,\n",
      "    \"p\": 0.6694478259886981,\n",
      "    \"r\": 0.7843821588672149\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.5832536548042484,\n",
      "    \"p\": 0.5520415420359586,\n",
      "    \"r\": 0.6355145644112287\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3679632768543453,\n",
      "    \"p\": 0.3580297861547844,\n",
      "    \"r\": 0.4217402027281053\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.43513239568044154,\n",
      "    \"p\": 0.40460253914202243,\n",
      "    \"r\": 0.5148282233780977\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.13306449323023853,\n",
      "    \"p\": 0.1253111707771059,\n",
      "    \"r\": 0.15949866729357143\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3742168715228249,\n",
      "    \"p\": 0.37411137585118187,\n",
      "    \"r\": 0.4766178844478526\n",
      "  }\n",
      "}\n",
      "\n",
      "scratch\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_scratch_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_scratch_d.txt\n",
      "BLEU = 31.83, 84.0/64.2/33.9/13.1 (BP=0.809, ratio=0.825, hyp_len=81660, ref_len=98982)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 8.58, 53.3/18.6/7.1/3.0 (BP=0.712, ratio=0.746, hyp_len=382932, ref_len=513062)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.7033337175189962,\n",
      "    \"p\": 0.6569318004272444,\n",
      "    \"r\": 0.7883652121204067\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.5726594140586921,\n",
      "    \"p\": 0.5380792226811041,\n",
      "    \"r\": 0.63061569643091\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.36388863108905184,\n",
      "    \"p\": 0.35274222110077064,\n",
      "    \"r\": 0.42931270570749863\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.4327693417114682,\n",
      "    \"p\": 0.40400226320141425,\n",
      "    \"r\": 0.5135144065148897\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.1341134225308634,\n",
      "    \"p\": 0.12737761139451662,\n",
      "    \"r\": 0.1610036571952838\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3701043131285504,\n",
      "    \"p\": 0.3726439469373078,\n",
      "    \"r\": 0.4742327413061736\n",
      "  }\n",
      "}\n",
      "\n",
      "345M\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_345M_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_345M_d.txt\n",
      "BLEU = 33.32, 84.3/64.7/34.1/13.0 (BP=0.844, ratio=0.855, hyp_len=84621, ref_len=98982)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 8.29, 53.6/18.4/6.9/2.9 (BP=0.698, ratio=0.736, hyp_len=377432, ref_len=513062)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.7195670239575412,\n",
      "    \"p\": 0.6784074129565094,\n",
      "    \"r\": 0.7916476242319681\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.5899175210943156,\n",
      "    \"p\": 0.5606969855188995,\n",
      "    \"r\": 0.6388975519563131\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3767264779001005,\n",
      "    \"p\": 0.3670142217361383,\n",
      "    \"r\": 0.43138777362499403\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.431068522577988,\n",
      "    \"p\": 0.4002837284876058,\n",
      "    \"r\": 0.514108244254165\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.13100549908785342,\n",
      "    \"p\": 0.12337887495286251,\n",
      "    \"r\": 0.15818309240390227\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.370268112561026,\n",
      "    \"p\": 0.3705070995459231,\n",
      "    \"r\": 0.47652944227271815\n",
      "  }\n",
      "}\n",
      "\n",
      "time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "for tag in data.gens:\n",
    "    print(tag)\n",
    "    data.gen = tag\n",
    "    data.to_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [10:50<00:00,  7.91it/s]\n",
      "  0%|          | 1/4000 [00:00<09:13,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7578521365259253\n",
      "average ingr number 7.81975\n",
      "scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [10:28<00:00,  7.92it/s]\n",
      "  0%|          | 1/4000 [00:00<09:28,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7463840283654203\n",
      "average ingr number 7.594\n",
      "345M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [10:42<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7668857387552463\n",
      "average ingr number 7.88125\n",
      "time: 32min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for tag in data.gens:\n",
    "    print(tag)\n",
    "    data.gen = tag\n",
    "    data.ingr_f1(root=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [24:10<00:00,  3.20it/s]\n",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5197623961367492\n",
      "scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [24:04<00:00,  3.04it/s]\n",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5231892865324351\n",
      "345M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [24:16<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5229560768790594\n",
      "time: 1h 12min 30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for tag in data.gens:\n",
    "    print(tag)\n",
    "    data.gen = tag\n",
    "    data.instr_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_k1_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_k1_d.txt\n",
      "BLEU = 33.51, 87.7/69.7/37.7/14.9 (BP=0.779, ratio=0.800, hyp_len=79877, ref_len=99866)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 9.81, 57.4/22.7/10.0/4.9 (BP=0.618, ratio=0.675, hyp_len=350990, ref_len=519837)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.7341866687662248,\n",
      "    \"p\": 0.6726128059329562,\n",
      "    \"r\": 0.8351929432186476\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.614198945619956,\n",
      "    \"p\": 0.5677043809518002,\n",
      "    \"r\": 0.6887721941405892\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.38415521113924106,\n",
      "    \"p\": 0.3666105804444019,\n",
      "    \"r\": 0.45770450963953724\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.45645021445471756,\n",
      "    \"p\": 0.4053629007109003,\n",
      "    \"r\": 0.5723580440985074\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.16248296376900637,\n",
      "    \"p\": 0.14431170031566243,\n",
      "    \"r\": 0.21008110920233866\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3895465270615702,\n",
      "    \"p\": 0.37759580451582564,\n",
      "    \"r\": 0.5335909671981874\n",
      "  }\n",
      "}\n",
      "\n",
      "k3\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_k3_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_k3_d.txt\n",
      "BLEU = 32.51, 83.6/63.8/33.5/12.9 (BP=0.834, ratio=0.847, hyp_len=84547, ref_len=99866)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 8.29, 53.8/18.5/6.9/2.9 (BP=0.701, ratio=0.738, hyp_len=383488, ref_len=519837)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.7104682856591087,\n",
      "    \"p\": 0.6676929879952465,\n",
      "    \"r\": 0.784187765928176\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.5806588502091046,\n",
      "    \"p\": 0.5498238087668117,\n",
      "    \"r\": 0.6331864507157513\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3720255249450792,\n",
      "    \"p\": 0.36213941264732946,\n",
      "    \"r\": 0.4273059216436654\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.4345200513698718,\n",
      "    \"p\": 0.4035466632609851,\n",
      "    \"r\": 0.5169092447340495\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.13216632558333702,\n",
      "    \"p\": 0.1241929021862522,\n",
      "    \"r\": 0.15999920275464133\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.37243839207840496,\n",
      "    \"p\": 0.3730987756873055,\n",
      "    \"r\": 0.4778795430551708\n",
      "  }\n",
      "}\n",
      "\n",
      "k5\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_k5_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_k5_d.txt\n",
      "BLEU = 32.58, 81.6/61.6/32.1/12.3 (BP=0.867, ratio=0.875, hyp_len=87426, ref_len=99866)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 7.81, 51.4/16.8/5.9/2.3 (BP=0.748, ratio=0.775, hyp_len=402676, ref_len=519837)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.7014687529213953,\n",
      "    \"p\": 0.6678812696165299,\n",
      "    \"r\": 0.7621236395188328\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.5714539149033933,\n",
      "    \"p\": 0.5480148493582745,\n",
      "    \"r\": 0.613740660496786\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3654910197687842,\n",
      "    \"p\": 0.3597985698073288,\n",
      "    \"r\": 0.41056446536504104\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.42259476168417154,\n",
      "    \"p\": 0.4000786735355376,\n",
      "    \"r\": 0.49199683886436585\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.12152856085971123,\n",
      "    \"p\": 0.11680989487789396,\n",
      "    \"r\": 0.1437181762401597\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.36311010646514763,\n",
      "    \"p\": 0.3692580060328357,\n",
      "    \"r\": 0.45392471867764345\n",
      "  }\n",
      "}\n",
      "\n",
      "k10\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_k10_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_k10_d.txt\n",
      "BLEU = 32.19, 77.7/57.5/29.6/11.2 (BP=0.923, ratio=0.925, hyp_len=92424, ref_len=99866)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 7.42, 48.1/14.9/4.9/1.9 (BP=0.826, ratio=0.839, hyp_len=436311, ref_len=519837)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.6818265562748185,\n",
      "    \"p\": 0.6635932755237539,\n",
      "    \"r\": 0.7234810769737484\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.5512270031374469,\n",
      "    \"p\": 0.5405116617393947,\n",
      "    \"r\": 0.5781947285542808\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3519743187040844,\n",
      "    \"p\": 0.353048378165955,\n",
      "    \"r\": 0.3851277357964655\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.4097360330041745,\n",
      "    \"p\": 0.4014140171609618,\n",
      "    \"r\": 0.4614854434663416\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.11255947748132306,\n",
      "    \"p\": 0.11230123967531092,\n",
      "    \"r\": 0.12859177427440957\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.35251190262701854,\n",
      "    \"p\": 0.37022190953104456,\n",
      "    \"r\": 0.4255998549479423\n",
      "  }\n",
      "}\n",
      "\n",
      "k30\n",
      "saved ../../to_gpt2/generation_ori_i.txt\n",
      "saved ../../to_gpt2/generation_k30_i.txt\n",
      "saved ../../to_gpt2/generation_ori_d.txt\n",
      "saved ../../to_gpt2/generation_k30_d.txt\n",
      "BLEU = 30.97, 72.0/51.6/26.0/9.5 (BP=1.000, ratio=1.002, hyp_len=100106, ref_len=99866)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "BLEU = 7.15, 44.3/12.9/4.0/1.5 (BP=0.936, ratio=0.938, hyp_len=487712, ref_len=519837)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.6508525255240014,\n",
      "    \"p\": 0.6561244263046886,\n",
      "    \"r\": 0.6681584202096891\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.5197185010055754,\n",
      "    \"p\": 0.5268803103221528,\n",
      "    \"r\": 0.5286224511178258\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.33538687692819924,\n",
      "    \"p\": 0.34916161977251814,\n",
      "    \"r\": 0.3553136783767815\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.3925300571204598,\n",
      "    \"p\": 0.4033824568089853,\n",
      "    \"r\": 0.4230892763992931\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.10137255736699351,\n",
      "    \"p\": 0.10629072574850898,\n",
      "    \"r\": 0.11095701942726463\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.33769196356311815,\n",
      "    \"p\": 0.3716179917430668,\n",
      "    \"r\": 0.389878484745952\n",
      "  }\n",
      "}\n",
      "\n",
      "time: 6min 34s\n"
     ]
    }
   ],
   "source": [
    "for tag in data.gens:\n",
    "    print(tag)\n",
    "    data.gen = tag\n",
    "    data.to_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [23:28<00:00,  4.55it/s]\n",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5115819243870295\n",
      "k3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [24:38<00:00,  4.39it/s]\n",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5230231769919328\n",
      "k5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [25:20<00:00,  5.00it/s]\n",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5278224521629334\n",
      "k10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [26:05<00:00,  3.79it/s]\n",
      "  0%|          | 1/4000 [00:00<10:03,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333886852206743\n",
      "k30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [28:09<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5401450212720956\n",
      "time: 2h 7min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for tag in data.gens:\n",
    "    print(tag)\n",
    "    data.gen = tag\n",
    "    data.instr_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
