{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 17.4 µs\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 36.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os, tqdm, sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.preprocessing import load\n",
    "from utils.words import make_corpus_0, get_wordcount_list\n",
    "from utils.save import make_dir, save_pickle, load_pickle, auto_save_csv, print_time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inflect\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import copy\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist\n",
      "drop 46 recipes with less than 2 ingredients\n",
      "furthur drop 1026 recipes with less than 2 instructions\n",
      "drop 0 recipes with no description\n",
      "now we are using recipe54k 54076\n",
      "time: 2.59 s\n"
     ]
    }
   ],
   "source": [
    "dic = load(dir_save = '../big_data/dic_20190830.pickle')\n",
    "ls = [i for i,v in dic.items() if len(v['ingredients'])>1]\n",
    "print('drop %d recipes with less than 2 ingredients' %(len(dic)-len(ls)))\n",
    "ls = [i for i in ls if len(dic[i]['directions'])>1]\n",
    "print('furthur drop %d recipes with less than 2 instructions' %(len(dic)-len(ls)))\n",
    "desc = [i for i in ls if len(dic[i]['description'])<1]\n",
    "print('drop %d recipes with no description' %(len(desc)))\n",
    "print('now we are using recipe54k %d' % len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preheat preheat VERB VB\n",
      "------------------------------\n",
      "oven | oven | oven\n",
      "350 degrees | degrees | degree\n",
      "f ( 175 degrees c | c | c\n",
      "==============================\n",
      "line line VERB VB\n",
      "------------------------------\n",
      "a baking sheet | sheet | sheet\n",
      "a sheet | sheet | sheet\n",
      "aluminum foil | foil | foil\n",
      "==============================\n",
      "stir stir VERB VB\n",
      "------------------------------\n",
      "ketchup | ketchup | ketchup\n",
      "mustard | mustard | mustard\n",
      "a plate | plate | plate\n",
      "==============================\n",
      "place place VERB VB\n",
      "------------------------------\n",
      "the cornflake crumbs | crumbs | crumb\n",
      "a shallow bowl | bowl | bowl\n",
      "==============================\n",
      "roll roll VERB VB\n",
      "roll roll VERB VB\n",
      "------------------------------\n",
      "each hot dog | dog | dog\n",
      "the ketchup mixture | mixture | mixture\n",
      "the cornflake crumbs | crumbs | crumb\n",
      "coat | coat | coat\n",
      "==============================\n",
      "------------------------------\n",
      "place | place | place\n",
      "prepared baking sheet | sheet | sheet\n",
      "==============================\n",
      "bake bake VERB VB\n",
      "are be VERB VBP\n",
      "crispy crispy VERB VBN\n",
      "------------------------------\n",
      "preheated oven | oven | oven\n",
      "the hot dogs | dogs | dog\n",
      "the inside | inside | inside\n",
      "the outside | outside | outside\n",
      "15 to 20 minutes | minutes | minute\n",
      "==============================\n",
      "time: 9.52 s\n"
     ]
    }
   ],
   "source": [
    "### example of spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "directions = ['preheat oven to 350 degrees f ( 175 degrees c ) .',\n",
    " 'line a baking sheet with a sheet of aluminum foil .',\n",
    " 'stir together ketchup and mustard on a plate until mixed .',\n",
    " 'place the cornflake crumbs in a shallow bowl .',\n",
    " 'roll each hot dog in the ketchup mixture , then roll in the cornflake crumbs to coat .',\n",
    " 'place onto prepared baking sheet .',\n",
    " 'bake in preheated oven until the hot dogs are hot on the inside , and crispy on the outside , 15 to 20 minutes .']\n",
    "\n",
    "for sent in directions:\n",
    "    doc = nlp(sent)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            print(token.text, token.lemma_, token.pos_, token.tag_)\n",
    "    print('-'*30)\n",
    "    for chunk in doc.noun_chunks:\n",
    "        root_lemma = [token.lemma_ for token in doc if token.text == chunk.root.text][0]\n",
    "        print(chunk.text,'|', chunk.root.text,'|', root_lemma)\n",
    "        \n",
    "    print('='*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### goal 1 : calculate precision and recall of verb and noun with spacy\n",
    "### goal 2 : further examine the precision and recall for ingredients and non-ingredients (via NY time parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.4 ms\n"
     ]
    }
   ],
   "source": [
    "class spacy_single_sent:\n",
    "    def __init__(self, sent):\n",
    "        '''\n",
    "        Args: sent: string\n",
    "        '''\n",
    "        self.doc = nlp(sent)\n",
    "    def noun(self):\n",
    "        noun = []\n",
    "        for chunk in self.doc.noun_chunks:\n",
    "            noun.append([token.lemma_ for i, token in enumerate(self.doc) if i == chunk.end-1][0])\n",
    "            #noun.append([token.text for i, token in enumerate(self.doc) if i == chunk.end-1][0]) # show original text\n",
    "        return noun\n",
    "    def verb(self):\n",
    "        return [token.lemma_ for token in self.doc if token.pos_ == 'VERB']\n",
    "\n",
    "def spacy_sentences(directions):\n",
    "    '''\n",
    "    Args: directions: list of strings\n",
    "    '''\n",
    "    nouns, verbs = [], []\n",
    "    for sent in directions:\n",
    "        read_sent = spacy_single_sent(sent)\n",
    "        nouns += read_sent.noun()\n",
    "        verbs += read_sent.verb()\n",
    "    return nouns, verbs\n",
    "\n",
    "class metrics:\n",
    "    def __init__(self, list_true, list_pred):\n",
    "        self.y_pred = list_pred\n",
    "        self.y_true = list_true\n",
    "        \n",
    "    # frequency weighted\n",
    "    def f1_freq(self):\n",
    "        precision = self.precision_freq()\n",
    "        recall = self.recall_freq()\n",
    "        return precision*recall/ 2*(precision + recall)\n",
    "    def precision_freq(self):\n",
    "        return self.scoring(self.y_pred, self.y_true)\n",
    "    def recall_freq(self):\n",
    "        return self.scoring(self.y_true, self.y_pred)\n",
    "    def scoring(self, n1, n2):\n",
    "        n1c, n2c = copy.deepcopy(n1), copy.deepcopy(n2)\n",
    "        score = 0\n",
    "        for word in n1c:\n",
    "            if word in n2c:\n",
    "                score +=1\n",
    "                n2c.remove(word)\n",
    "        if len(n1c): \n",
    "            return score/len(n1c)\n",
    "    # without frequency weighted\n",
    "    def precision(self):\n",
    "        if len(self.y_pred): \n",
    "            return len(set(self.y_true) & set(self.y_pred))/len(set(self.y_pred))\n",
    "    def recall(self):\n",
    "        if len(self.y_true):\n",
    "            return len(set(self.y_true) & set(self.y_pred))/len(set(self.y_true))\n",
    "    def f1(self):\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "        return precision*recall/ 2*(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "filename = '../../to_gpt2/generation_333k_sorted/'\n",
    "def load_dir_data(filename):\n",
    "    ls = []\n",
    "    if os.path.isdir(filename):\n",
    "        # Directory\n",
    "        for (dirpath, _, fnames) in os.walk(filename):\n",
    "            for fname in fnames:\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r') as fp:\n",
    "                    raw_text = fp.read()\n",
    "                # if it contain instr\n",
    "                if fname[-5] == 'd':\n",
    "                    dic[int(fname[:-5])]['generated_instr'] = raw_text.split('.')\n",
    "                    ls.append(int(fname[:-5]))# only interested in instr\n",
    "                # if it contain ingred\n",
    "                if fname[-5] == 'i':\n",
    "                    dic[int(fname[:-5])]['generated_ingred'] = raw_text.split('$')\n",
    "                # if it contain name\n",
    "                if fname[-5] == 't':\n",
    "                    dic[int(fname[:-5])]['generated_name'] = raw_text\n",
    "                    \n",
    "    return sorted(list(set(ls)))\n",
    "ls = load_dir_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55102/55102 [01:39<00:00, 554.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "noun_recall         0.491854\n",
       "noun_recall_f       0.445608\n",
       "noun_precision      0.606065\n",
       "noun_precision_f    0.521618\n",
       "verb_recall         0.312607\n",
       "verb_recall_f       0.291760\n",
       "verb_precision      0.372829\n",
       "verb_precision_f    0.337052\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        n_pred, v_pred = spacy_sentences(dic[i]['generated_instr'])\n",
    "        n_true, v_true = spacy_sentences(dic[i]['directions'])\n",
    "        score = metrics(n_true, n_pred)\n",
    "        scores = {'noun_recall':score.recall(), 'noun_recall_f':score.recall_freq(),\n",
    "                  'noun_precision':score.precision(), 'noun_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        score = metrics(v_true, v_pred)\n",
    "        scores = {'verb_recall':score.recall(), 'verb_recall_f':score.recall_freq(),\n",
    "                  'verb_precision':score.precision(), 'verb_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "df2[[col for col in df2.columns if 'recall' in col or 'precision' in col]].iloc[ls].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 87.2 ms\n"
     ]
    }
   ],
   "source": [
    "filename = '../../to_gpt2/generation_333k/'\n",
    "ls = load_dir_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.6 ms\n"
     ]
    }
   ],
   "source": [
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 8 top-k sampling (k=3)\n",
    "filename = '../../to_gpt2/generation_333k/'\n",
    "ls = load_dir_data(filename)\n",
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        n_pred, v_pred = spacy_sentences(dic[i]['generated_instr'])\n",
    "        n_true, v_true = spacy_sentences(dic[i]['directions'])\n",
    "        score = metrics(n_true, n_pred)\n",
    "        scores = {'noun_recall':score.recall(), 'noun_recall_f':score.recall_freq(),\n",
    "                  'noun_precision':score.precision(), 'noun_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        score = metrics(v_true, v_pred)\n",
    "        scores = {'verb_recall':score.recall(), 'verb_recall_f':score.recall_freq(),\n",
    "                  'verb_precision':score.precision(), 'verb_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "df2[[col for col in df2.columns if 'recall' in col or 'precision' in col]].iloc[ls].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55102/55102 [01:42<00:00, 538.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "noun_recall         0.453282\n",
       "noun_recall_f       0.412267\n",
       "noun_precision      0.587857\n",
       "noun_precision_f    0.507792\n",
       "verb_recall         0.330381\n",
       "verb_recall_f       0.314204\n",
       "verb_precision      0.372977\n",
       "verb_precision_f    0.336897\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "# batch = 8 top-k sampling (k=3) verb *5\n",
    "# batch = 8 version\n",
    "filename = '../../to_gpt2/generation_358k/'\n",
    "ls = load_dir_data(filename)\n",
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        n_pred, v_pred = spacy_sentences(dic[i]['generated_instr'])\n",
    "        n_true, v_true = spacy_sentences(dic[i]['directions'])\n",
    "        score = metrics(n_true, n_pred)\n",
    "        scores = {'noun_recall':score.recall(), 'noun_recall_f':score.recall_freq(),\n",
    "                  'noun_precision':score.precision(), 'noun_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        score = metrics(v_true, v_pred)\n",
    "        scores = {'verb_recall':score.recall(), 'verb_recall_f':score.recall_freq(),\n",
    "                  'verb_precision':score.precision(), 'verb_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "df2[[col for col in df2.columns if 'recall' in col or 'precision' in col]].iloc[ls].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55102/55102 [01:52<00:00, 488.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "noun_recall         0.501185\n",
       "noun_recall_f       0.442208\n",
       "noun_precision      0.580863\n",
       "noun_precision_f    0.501342\n",
       "verb_recall         0.293824\n",
       "verb_recall_f       0.279292\n",
       "verb_precision      0.342315\n",
       "verb_precision_f    0.299594\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "# i-prone version (old but has a high F1)\n",
    "filename = '../../to_gpt2/generation_1221k_topk/'\n",
    "ls = load_dir_data(filename)\n",
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        n_pred, v_pred = spacy_sentences(dic[i]['generated_instr'])\n",
    "        n_true, v_true = spacy_sentences(dic[i]['directions'])\n",
    "        score = metrics(n_true, n_pred)\n",
    "        scores = {'noun_recall':score.recall(), 'noun_recall_f':score.recall_freq(),\n",
    "                  'noun_precision':score.precision(), 'noun_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        score = metrics(v_true, v_pred)\n",
    "        scores = {'verb_recall':score.recall(), 'verb_recall_f':score.recall_freq(),\n",
    "                  'verb_precision':score.precision(), 'verb_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic[i].update(scores)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "df2[[col for col in df2.columns if 'recall' in col or 'precision' in col]].iloc[ls].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        ingredients = [sent.split(' ') for sent in v['ingredients']]\n",
    "        dic[i]['ingred_level2'] = [model.equal_check(ingred) for ingred in ingredients]\n",
    "        generated_ingredients = [sent.split(' ') for sent in v['generated_ingred']]\n",
    "        \n",
    "        dic[i]['generated_ingredients_level2'] = [model.equal_check(instr) for instr in generated_ingredients]\n",
    "        dic[i]['anymatched_recall'] = anymatched_recall(dic[i]['ingred_level2'], generated_ingredients)\n",
    "        dic[i]['multiframe_recall'] = multiframe_recall(dic[i]['ingred_level2'], generated_ingredients)\n",
    "        dic[i]['multiframe_precision'] = multiframe_recall(dic[i]['generated_ingredients_level2'], ingredients)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "df2[['anymatched_recall','multiframe_recall','multiframe_precision']].iloc[ls].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = 8 top-k sampling (k=3)\n",
    "filename = '../../to_gpt2/generation_333k/'\n",
    "ls = load_dir_data(filename)\n",
    "\n",
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        ingredients = [sent.split(' ') for sent in v['ingredients']]\n",
    "        dic[i]['ingred_level2'] = [model.equal_check(ingred) for ingred in ingredients]\n",
    "        generated_ingredients = [sent.split(' ') for sent in v['generated_ingred']]\n",
    "        \n",
    "        dic[i]['generated_ingredients_level2'] = [model.equal_check(instr) for instr in generated_ingredients]\n",
    "        dic[i]['anymatched_recall'] = anymatched_recall(dic[i]['ingred_level2'], generated_ingredients)\n",
    "        dic[i]['multiframe_recall'] = multiframe_recall(dic[i]['ingred_level2'], generated_ingredients)\n",
    "        dic[i]['multiframe_precision'] = multiframe_recall(dic[i]['generated_ingredients_level2'], ingredients)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "df2[['anymatched_recall','multiframe_recall','multiframe_precision']].iloc[ls].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evlautate wang's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54507, 54507)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 155 ms\n"
     ]
    }
   ],
   "source": [
    "def read_text_files(filename):\n",
    "    raw_text = {}\n",
    "    if os.path.isdir(filename):\n",
    "        # Directory\n",
    "        for (dirpath, _, fnames) in os.walk(filename):\n",
    "            for fname in fnames:\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r') as fp:\n",
    "                    raw_text[fname] = fp.read().split('\\n')\n",
    "    return raw_text\n",
    "\n",
    "filename = '../../to_gpt2/wang/'\n",
    "raw_text = read_text_files(filename)\n",
    "len(raw_text['generated_instructions_bert_47.txt']), len(raw_text['gt_instructions.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_evaluate = {}\n",
    "list_gen, list_gt = raw_text['generated_instructions_bert_47.txt'], raw_text['gt_instructions.txt']\n",
    "for i, gen in tqdm.tqdm(enumerate(list_gen)):\n",
    "        n_pred, v_pred = spacy_sentences(gen.split('.'))\n",
    "        n_true, v_true = spacy_sentences(list_gt[i].split('.'))\n",
    "        score = metrics(n_true, n_pred)\n",
    "        scores = {'noun_recall':score.recall(), 'noun_recall_f':score.recall_freq(),\n",
    "                  'noun_precision':score.precision(), 'noun_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic_evaluate[i]=scores\n",
    "        score = metrics(v_true, v_pred)\n",
    "        scores = {'verb_recall':score.recall(), 'verb_recall_f':score.recall_freq(),\n",
    "                  'verb_precision':score.precision(), 'verb_precision_f':score.precision_freq(),\n",
    "                 }\n",
    "        dic_evaluate[i].update(scores)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic_evaluate, orient = 'index')\n",
    "df2[[col for col in df2.columns if 'recall' in col or 'precision' in col]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### examine error cases in NYtimes parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.45 s\n"
     ]
    }
   ],
   "source": [
    "dir_save2 = os.path.normpath(dir_HugeFiles+'preprocessing/ingred_20190607.txt')\n",
    "overwrite = False\n",
    "with open(dir_save2,'w') as f:\n",
    "    list_idid = []\n",
    "    for i, value in dic.items():\n",
    "        for j, ingred in enumerate(value['ingredients']):\n",
    "            #assert type(ingred[0]) == str\n",
    "            #assert ingred != \"\"\n",
    "            if not ingred == '' and ' ':\n",
    "                pair = [i, j, ingred]\n",
    "                list_idid.append(pair)\n",
    "                if overwrite:\n",
    "                    f.write(\"%s\\n\" % ingred)\n",
    "def clean_ny(row):\n",
    "    '''\n",
    "    Args:\n",
    "        row: list of lines\n",
    "    '''\n",
    "    return [ele for ele in row if type(ele)==str and len(ele)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use python2 <br>\n",
    "cd /home/helena/NYtime-parser2/ <br>\n",
    "python bin/parse-ingredients.py /data/yueliu/RecipeAnalytics_201902/preprocessing/ingred_20190302.txt > results_20190302.txt <br>\n",
    "python bin/convert-to-json.py results_20190302.txt > results_20190302.json <br>\n",
    "move the results_20190302.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save2 = os.path.normpath('/data/yueliu/RecipeAnalytics_201902/preprocessing/results_20190302.json')\n",
    "df_idid = pd.DataFrame(list_idid, columns = ['index_1','index_2','ingred'])\n",
    "parsed = pd.read_json(dir_save2)\n",
    "df_idid[['input','qty','unit','name','other']] =  parsed[['input','qty','unit','name','other']]\n",
    "df_parsed = df_idid.groupby('index_1')[['name']].agg(lambda x: list(x)).reset_index(drop = True)\n",
    "for i, value in dic.items():\n",
    "    dic[i]['ingred_ny'] = clean_ny(df_parsed.iloc[i]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_1</th>\n",
       "      <th>index_2</th>\n",
       "      <th>ingred</th>\n",
       "      <th>input</th>\n",
       "      <th>qty</th>\n",
       "      <th>unit</th>\n",
       "      <th>name</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1 ( 16 ounce ) package all beef hot dogs</td>\n",
       "      <td>1 (16 ounce) package all beef hot dogs</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>1 ( 4 ounce ) package white anchovies , washed</td>\n",
       "      <td>1 (4 ounce) package white anchovies, washed</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>1 ( 4 ounce ) package white anchovies , washed</td>\n",
       "      <td>1 (4 ounce) package white anchovies, washed</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index_1  index_2                                          ingred  \\\n",
       "3         0        3        1 ( 16 ounce ) package all beef hot dogs   \n",
       "48        6        5                                   salt to taste   \n",
       "56        6       13  1 ( 4 ounce ) package white anchovies , washed   \n",
       "63        6       20  1 ( 4 ounce ) package white anchovies , washed   \n",
       "64        6       21                                   salt to taste   \n",
       "\n",
       "                                          input   qty unit name other  \n",
       "3        1 (16 ounce) package all beef hot dogs     1  NaN  NaN   NaN  \n",
       "48                                salt to taste  salt  NaN  NaN   NaN  \n",
       "56  1 (4 ounce) package white anchovies, washed     1  NaN  NaN   NaN  \n",
       "63  1 (4 ounce) package white anchovies, washed     1  NaN  NaN   NaN  \n",
       "64                                salt to taste  salt  NaN  NaN   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index_1</th>\n",
       "      <th>index_2</th>\n",
       "      <th>ingred</th>\n",
       "      <th>input</th>\n",
       "      <th>qty</th>\n",
       "      <th>unit</th>\n",
       "      <th>name</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>2 cups half and half</td>\n",
       "      <td>2 cups half and half</td>\n",
       "      <td>2</td>\n",
       "      <td>cup</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1 avocado peeled , pitted , and cut into chunks</td>\n",
       "      <td>1 avocado peeled, pitted, and cut into chunks</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt to taste</td>\n",
       "      <td>salt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index_1  index_2                                           ingred  \\\n",
       "48         6        5                                    salt to taste   \n",
       "64         6       21                                    salt to taste   \n",
       "110       11       13                             2 cups half and half   \n",
       "275       30        0  1 avocado peeled , pitted , and cut into chunks   \n",
       "315       35        4                                    salt to taste   \n",
       "\n",
       "                                             input   qty unit name other  \n",
       "48                                   salt to taste  salt  NaN  NaN   NaN  \n",
       "64                                   salt to taste  salt  NaN  NaN   NaN  \n",
       "110                           2 cups half and half     2  cup  NaN   NaN  \n",
       "275  1 avocado peeled, pitted, and cut into chunks     1  NaN  NaN     ,  \n",
       "315                                  salt to taste  salt  NaN  NaN   NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "df = df_idid[df_idid['name'].isna()]\n",
    "display(df.head())\n",
    "display(df[df['ingred'].apply(lambda x: '(' not in x)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO \n",
    "### delete the words within braces\n",
    "### use NY-times parser\n",
    "### manually fix error cases such as \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29.3 ms\n"
     ]
    }
   ],
   "source": [
    "p = inflect.engine()\n",
    "class closest:\n",
    "    def __init__(self, series):\n",
    "        find_ngram = series.apply(lambda x: ' ' in x)\n",
    "        self.unigram_ingredients, self.ngram_ingredients = series[~find_ngram], series[find_ngram]\n",
    "        # self.unigram_ingredients_pluraled = [p.plural(ingr) for ingr in self.unigram_ingredients]\n",
    "    \n",
    "    def ngram(self, listofline, pluraled_listofline):\n",
    "        answer = []\n",
    "        for ngram in self.ngram_ingredients.values:\n",
    "            n_list= ngram.split(' ')\n",
    "            intersection = set(listofline) & set(n_list)\n",
    "            if len(intersection) >= 2 and len(intersection) == len(n_list):\n",
    "                answer.append(ngram)\n",
    "            else:\n",
    "                # repeat\n",
    "                intersection = (set(pluraled_listofline) | set(intersection)) & set(n_list)\n",
    "                if len(intersection) >= 2 and len(intersection) == len(n_list):\n",
    "                    answer.append(ngram)\n",
    "        return set(answer)\n",
    "    \n",
    "    def unigram(self, listofline, pluraled_listofline):\n",
    "        first =  set(listofline) & set(self.unigram_ingredients)\n",
    "        if first:\n",
    "            return first\n",
    "        second = set(pluraled_listofline) & set(self.unigram_ingredients)\n",
    "        if second:\n",
    "            return second\n",
    "        \n",
    "    def check(self, listofline):\n",
    "        '''\n",
    "        assign higher priority to n-grams over uni-gram\n",
    "        '''\n",
    "        pluraled_listofline = [p.plural(word) for word in listofline]\n",
    "        ngram = self.ngram(listofline, pluraled_listofline)\n",
    "        if ngram:\n",
    "            return ngram\n",
    "        unigram = self.unigram(listofline, pluraled_listofline)\n",
    "        if unigram:\n",
    "            return unigram\n",
    "        \n",
    "    def equal_check(self, listofline):\n",
    "        '''\n",
    "        equally care about unigram and n gram\n",
    "        '''\n",
    "        pluraled_listofline = [p.plural(word) for word in listofline]\n",
    "        ngram = self.ngram(listofline, pluraled_listofline)\n",
    "        unigram = self.unigram(listofline, pluraled_listofline)\n",
    "        if ngram and unigram:\n",
    "            return ngram | unigram\n",
    "        if ngram:\n",
    "            return ngram\n",
    "        if unigram:\n",
    "            return unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class closest_lighter(closest):\n",
    "    def __init__(self, set_grams):\n",
    "        self.unigram_ingredients = pd.Series([x for x in set_grams if ' ' not in x])\n",
    "        self.ngram_ingredients = pd.Series([x for x in set_grams if ' ' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.4 ms\n"
     ]
    }
   ],
   "source": [
    "# prepare the ingredient detection algorithm\n",
    "model = closest(df[2])\n",
    "def multiframe_recall(ingred_level2, directions):\n",
    "    '''\n",
    "    ingred_level2: dic[i]['ingred_level2']; list of set; ground truth;\n",
    "    directions: dic[i]['directions']; list of words; prediciton\n",
    "    denominator: number of identified n-grams\n",
    "    '''\n",
    "    try:\n",
    "        # recall \n",
    "        true_flatten = set.union(*[line for line in ingred_level2 if line])\n",
    "        model_lighter = closest_lighter(true_flatten)\n",
    "        direct_level2 = [model_lighter.equal_check(instr) for instr in directions]\n",
    "        pred_flatten = set.union(*[line for line in direct_level2 if line])\n",
    "        true_positive = true_flatten & pred_flatten\n",
    "\n",
    "        recall = len(true_positive)/len(true_flatten)\n",
    "        #precision = len(true_positive)/len(pred_flatten)\n",
    "        #f1 = 2* recall * precision / (recall + precision)\n",
    "        return recall #, precision, f1\n",
    "    except TypeError:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def anymatched_recall(ingred_level2, directions):\n",
    "    '''\n",
    "    ingred_level2: dic[i]['ingred_level2']; list of set; ground truth;\n",
    "    directions: dic[i]['directions']; list of words; prediciton\n",
    "    denominator: number of ingredients\n",
    "    '''\n",
    "    try:\n",
    "        # recall \n",
    "        true_flatten = set.union(*[line for line in ingred_level2 if line])\n",
    "        model_lighter = closest_lighter(true_flatten)\n",
    "        direct_level2 = [model_lighter.equal_check(instr) for instr in directions]\n",
    "        pred_flatten = set.union(*[line for line in direct_level2 if line])\n",
    "        true_positive = true_flatten & pred_flatten\n",
    "\n",
    "        recall = len(true_positive)/len(true_flatten)\n",
    "        return np.mean([1 if line & true_positive else 0 for line in ingred_level2 if line])\n",
    "    except TypeError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "filename = '../../to_gpt2/generation_308k/'\n",
    "def load_dir_data(filename):\n",
    "    ls = []\n",
    "    if os.path.isdir(filename):\n",
    "        # Directory\n",
    "        for (dirpath, _, fnames) in os.walk(filename):\n",
    "            for fname in fnames:\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r') as fp:\n",
    "                    raw_text = fp.read()\n",
    "                # if it contain instr\n",
    "                if fname[-5] == 'd':\n",
    "                    dic[int(fname[:-5])]['generated_instr'] = [raw_text]\n",
    "                # if it contain ingred\n",
    "                if fname[-5] == 'i':\n",
    "                    dic[int(fname[:-5])]['generated_ingred'] = raw_text.split('$')\n",
    "                # if it contain name\n",
    "                if fname[-5] == 'n':\n",
    "                    dic[int(fname[:-5])]['generated_name'] = raw_text\n",
    "                ls.append(int(fname[:-5]))\n",
    "    return sorted(list(set(ls)))\n",
    "ls = load_dir_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55102/55102 [00:45<00:00, 1200.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "anymatched_recall       0.498159\n",
       "multiframe_recall       0.427480\n",
       "multiframe_precision    0.304958\n",
       "dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        ingredients = [sent.split(' ') for sent in v['ingredients']]\n",
    "        dic[i]['ingred_level2'] = [model.equal_check(ingred) for ingred in ingredients]\n",
    "        generated_ingredients = [sent.split(' ') for sent in v['generated_ingred']]\n",
    "        \n",
    "        dic[i]['generated_ingredients_level2'] = [model.equal_check(instr) for instr in generated_ingredients]\n",
    "        dic[i]['anymatched_recall'] = anymatched_recall(dic[i]['ingred_level2'], generated_ingredients)\n",
    "        dic[i]['multiframe_recall'] = multiframe_recall(dic[i]['ingred_level2'], generated_ingredients)\n",
    "        dic[i]['multiframe_precision'] = multiframe_recall(dic[i]['generated_ingredients_level2'], ingredients)\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "df2[['anymatched_recall','multiframe_recall','multiframe_precision']].iloc[ls].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
