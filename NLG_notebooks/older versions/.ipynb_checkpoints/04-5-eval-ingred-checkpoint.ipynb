{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINS Task: overwriting (reusing) task id=87bcc6ea17524420bea9f1333d57b2da\n",
      "2019-09-27 16:35:54,744 - trains.Task - INFO - No repository found, storing script code instead\n",
      "TRAINS Monitor: GPU monitoring is not available, run \"pip install gpustat\"\n",
      "TRAINS results page: files_server: http://10.0.106.144:5909/projects/d7092e97e1174fb4bc1d441040d57553/experiments/87bcc6ea17524420bea9f1333d57b2da/output/log\n"
     ]
    }
   ],
   "source": [
    "from trains import Task\n",
    "task = Task.init(project_name=\"HelenaIngrF1\", task_name=\"default\")\n",
    "logger = Task.current_task().get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.3 µs\n",
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "time: 49.2 ms\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# if cannot import the modules, add the parent directory to system path might help\n",
    "\n",
    "import os, tqdm, sys\n",
    "parent_dir = os.path.abspath(os.getcwd()+'/..')+'/'\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.path import dir_HugeFiles\n",
    "from utils.preprocessing import load\n",
    "from utils.save import make_dir, save_pickle, load_pickle, save\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import copy\n",
    "import re\n",
    "random_seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist ../big_data/dic_20190830.pickle\n",
      "drop 46 recipes with less than 2 ingredients\n",
      "furthur drop 1026 recipes with less than 2 instructions\n",
      "drop 0 recipes with no description\n",
      "now we are using recipe54k 54076\n",
      "time: 2.99 s\n"
     ]
    }
   ],
   "source": [
    "dic = load(dir_save = '../big_data/dic_20190830.pickle')\n",
    "ls = [i for i,v in dic.items() if len(v['ingredients'])>1]\n",
    "print('drop %d recipes with less than 2 ingredients' %(len(dic)-len(ls)))\n",
    "ls = [i for i in ls if len(dic[i]['directions'])>1]\n",
    "print('furthur drop %d recipes with less than 2 instructions' %(len(dic)-len(ls)))\n",
    "desc = [i for i in ls if len(dic[i]['description'])<1]\n",
    "print('drop %d recipes with no description' %(len(desc)))\n",
    "print('now we are using recipe54k %d' % len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 45.9 ms\n"
     ]
    }
   ],
   "source": [
    "### STEP2 load and clean the generation\n",
    "\n",
    "def reverse(text):\n",
    "    '''\n",
    "    Important data cleaning before NY times parser\n",
    "    '''\n",
    "    # replace things in brace\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    # remove space before punct\n",
    "    text = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "    # remove consecutive spaces\n",
    "    text = re.sub(' +',' ',text).strip()\n",
    "    return text\n",
    "\n",
    "def reverse_list(listoftext):\n",
    "    output=[]\n",
    "    for text in listoftext:\n",
    "        rev = reverse(text)\n",
    "        if rev:\n",
    "            output.append(rev)\n",
    "    return output\n",
    "\n",
    "def load_dir_data(filename):\n",
    "    ls = []\n",
    "    if os.path.isdir(filename):\n",
    "        print('load', filename)\n",
    "        # Directory\n",
    "        for (dirpath, _, fnames) in os.walk(filename):\n",
    "            for fname in fnames:\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r') as fp:\n",
    "                    raw_text = fp.read()\n",
    "                    \n",
    "                # if it contains instr\n",
    "                if fname[-5] == 'd':\n",
    "                    dic[int(fname[:-5])]['generated_instr'] = reverse_list(raw_text.split('.'))\n",
    "\n",
    "                # if it contains ingred\n",
    "                if fname[-5] == 'i':\n",
    "                    dic[int(fname[:-5])]['generated_ingred'] = reverse_list(raw_text.split('$'))\n",
    "                    \n",
    "                # if it contains name\n",
    "                if fname[-5] == 't':\n",
    "                    dic[int(fname[:-5])]['generated_name'] = raw_text\n",
    "                ls.append(int(fname[:-5]))# only interested in instr\n",
    "                    \n",
    "    return sorted(list(set(ls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44.8 ms\n"
     ]
    }
   ],
   "source": [
    "### STEP3 sent to the NYtimes\n",
    "### assign indices to each ingredient <---> NYtimes\n",
    "class ny_ingredients:\n",
    "    def __init__(self, fields):\n",
    "        # this function will take the global variable ls and dic\n",
    "        # static & reuseable\n",
    "        self.ny_ingred = '../../NYtime-parser2/ingred.txt'\n",
    "        self.ny_result = '../../NYtime-parser2/result.json'\n",
    "        \n",
    "        # spacy\n",
    "        self.spacy = spacy.load('en_core_web_lg')\n",
    "        self.fields = fields #['ingredients', 'generated_ingred']\n",
    "\n",
    "    def to_ny(self):\n",
    "        '''\n",
    "        using global variables dic and ls\n",
    "        '''\n",
    "        to_write = []\n",
    "        for i, v in dic.items():\n",
    "            if i in ls:\n",
    "                # assing index\n",
    "                for field in self.fields:\n",
    "                    line_ids = []\n",
    "                    for line in v[field]:\n",
    "                        reversed_line = reverse(line)\n",
    "                        if line in to_write:\n",
    "                            ny_id = to_write.index(reversed_line)\n",
    "                        else:\n",
    "                            ny_id = len(to_write)\n",
    "                            to_write.append(reversed_line)\n",
    "                        line_ids.append(ny_id)\n",
    "                    dic[i]['ny_%s'%(field)] = line_ids\n",
    "\n",
    "        # save the file to the folder under NYtime-parser2\n",
    "        save(filename = self.ny_ingred, \n",
    "             to_write = '\\n'.join(to_write),\n",
    "             overwrite = True, \n",
    "             print_=True)\n",
    "\n",
    "        self.to_write = to_write\n",
    "        \n",
    "    # step 3\n",
    "    def to_ingred(self):\n",
    "        '''\n",
    "        using global variables dic and ls\n",
    "        '''\n",
    "        ny_result = pd.read_json(self.ny_result)\n",
    "        to_write = []\n",
    "        for i, v in dic.items():\n",
    "            if i in ls:\n",
    "                # assing index\n",
    "                for field in self.fields:\n",
    "                    temp = [ny_result.loc[ny_id]['name'] for ny_id in v['ny_%s'%(field)]]\n",
    "                    exact, root = self.extract(temp)\n",
    "                    dic[i]['ny_%s'%(field)] = {'ny':temp, 'exact':exact, 'root':root}\n",
    "                    \n",
    "    def extract(self, ny_ingred):\n",
    "        '''\n",
    "        Args: ny_ingred: a list of ingredient names\n",
    "        '''\n",
    "        phrases_to_sentences = ' '.join(['Mix the %s and water.'%ingr for ingr in ny_ingred])\n",
    "        doc = self.spacy(phrases_to_sentences)\n",
    "        exact_match, root_match = [],[]\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if chunk.text != 'water':\n",
    "                root_lemma = [token.lemma_ for token in doc if token.text == chunk.root.text][0]\n",
    "                exact_match.append(chunk.lemma_.replace('the ',''))\n",
    "                root_match.append(root_lemma)\n",
    "        return exact_match, root_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.9 s\n"
     ]
    }
   ],
   "source": [
    "for i, v in dic.items():\n",
    "    if i in ls:\n",
    "        for field in ['name', 'ingredients', 'directions']:\n",
    "            dic[i][field] = reverse_list(v[field])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### send the ground truth to ny parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ../../NYtime-parser2/ingred.txt\n",
      "time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "### start                    \n",
    "ny_ingr = ny_ingredients(fields = ['ingredients'])\n",
    "### step 3-1 save it as ingred.txt\n",
    "ny_ingr.to_ny()\n",
    "### step 3-2 go to python2 and run NLG_notebooks/Control Nytimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23min 31s\n"
     ]
    }
   ],
   "source": [
    "### step 3-3 load the result.json back to dic\n",
    "ny_ingr.to_ingred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "# for the field ingredients\n",
    "save_pickle(obj = dic, filename='../big_data/dic_20190927.pickle', overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### send the generated text to ny parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist ../big_data/dic_20190927.pickle\n",
      "time: 3.52 s\n"
     ]
    }
   ],
   "source": [
    "dic = load(dir_save = '../big_data/dic_20190927.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ../../to_gpt2/generation_333k_sorted/\n",
      "time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "filename = '../../to_gpt2/generation_333k_sorted/'\n",
    "ls = load_dir_data(filename)\n",
    "#filename = '../../to_gpt2/generation_1221k_top0.95_new/'\n",
    "#ls = load_dir_data(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ../../NYtime-parser2/ingred.txt\n",
      "time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "ny_ingr = ny_ingredients(fields = ['generated_ingred'])\n",
    "### step 3-1 save it as ingred.txt\n",
    "ny_ingr.to_ny()\n",
    "### step 3-2 go to python2 and run NLG_notebooks/Control Nytimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "### step 3-3 load the result.json back to dic\n",
    "ny_ingr.to_ingred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.6 ms\n"
     ]
    }
   ],
   "source": [
    "class metrics:\n",
    "    def __init__(self, list_true, list_pred):\n",
    "        self.y_true = [word for word in list_true if word != 'nan']\n",
    "        self.y_pred = [word for word in list_pred if word != 'nan']\n",
    "        self.y_true_string = ' %s '%(' '.join(self.y_true))\n",
    "        self.y_pred_string = ' %s '%(' '.join(self.y_pred))\n",
    "        # from collections import Counter\n",
    "        \n",
    "    # frequency weighted\n",
    "    def f1_freq(self):\n",
    "        precision = self.precision_freq()\n",
    "        recall = self.recall_freq()\n",
    "        try:\n",
    "            f1 = 2*precision*recall/(precision + recall)\n",
    "        except ZeroDivisionError:\n",
    "            f1 = 0\n",
    "        return f1\n",
    "    \n",
    "    def precision_freq(self):\n",
    "        return self.scoring(self.y_pred, self.y_true)\n",
    "    \n",
    "    def recall_freq(self):\n",
    "        return self.scoring(self.y_true, self.y_pred)\n",
    "    \n",
    "    def scoring(self, n1, n2):\n",
    "        n1c, n2c = copy.deepcopy(n1), copy.deepcopy(n2)\n",
    "        score = 0\n",
    "        for word in n1c:\n",
    "            if word in n2c:\n",
    "                score +=1\n",
    "                n2c.remove(word)\n",
    "        if len(n1c): \n",
    "            return score/len(n1c)\n",
    "        \n",
    "    # without frequency weighted\n",
    "    def precision(self):\n",
    "        if len(self.y_pred): \n",
    "            return len(set(self.y_true) & set(self.y_pred))/len(set(self.y_pred))\n",
    "    def recall(self):\n",
    "        if len(self.y_true):\n",
    "            return len(set(self.y_true) & set(self.y_pred))/len(set(self.y_true))\n",
    "    def f1(self):\n",
    "        precision = self.precision()\n",
    "        recall = self.recall()\n",
    "        try:\n",
    "            f1 = 2*precision*recall/(precision + recall)\n",
    "        except ZeroDivisionError:\n",
    "            f1 = 0\n",
    "        return f1\n",
    "    \n",
    "    # return scores in a dict    \n",
    "    def all_recall(self, name):\n",
    "        output = {}\n",
    "        output['recall_%s'%(name)] = self.recall()\n",
    "        output['recall_freq_%s'%(name)] = self.recall_freq()\n",
    "        return output\n",
    "    \n",
    "    def all_precision(self, name):\n",
    "        output = {}\n",
    "        output['precision_%s'%(name)] = self.precision()\n",
    "        output['precision_freq_%s'%(name)] = self.precision_freq()\n",
    "        return output\n",
    "    \n",
    "    # frequency weighted\n",
    "    def ngram_scoring(self, n1, n2):\n",
    "        '''\n",
    "        n1 = [' ddd d ',' der ',' w ',' w ']\n",
    "        n2 = ' ddd d mnnm,n,m der ddd d w ow '\n",
    "        '''\n",
    "        n1c, n2c = copy.deepcopy([' %s ' % word for word in n1]), copy.deepcopy(n2)\n",
    "        true_counter = Counter(n1)\n",
    "        denomintater = sum(true_counter.values())\n",
    "        score = 0\n",
    "        for word in true_counter:\n",
    "            occurrence = n2.count(word)\n",
    "            occurrence = occurrence if occurrence < true_counter[word] else true_counter[word]\n",
    "            score += occurrence\n",
    "        return score/sum(true_counter.values())\n",
    "    \n",
    "    def ngram_recall_freq(self):\n",
    "        return self.ngram_scoring(self.y_true, self.y_pred_string)\n",
    "    \n",
    "    def ngram_precision_freq(self):\n",
    "        return self.ngram_scoring(self.y_pred, self.y_true_string)\n",
    "    \n",
    "    #  unweighted\n",
    "    def ngram_recall(self):\n",
    "        return np.mean([True if word in self.y_pred_string else False for word in set(self.y_true)])\n",
    "\n",
    "    def ngram_precision(self):\n",
    "        return np.mean([True if word in self.y_true_string else False for word in set(self.y_pred)])\n",
    "    \n",
    "    def all_ngram_recall(self, name):\n",
    "        output = {}\n",
    "        output['recall_ngram_%s'%(name)] = self.ngram_recall()\n",
    "        output['recall_ngram_freq_%s'%(name)] = self.ngram_recall_freq()\n",
    "        return output\n",
    "    \n",
    "    def all_ngram_precision(self, name):\n",
    "        output = {}\n",
    "        output['precision_ngram_%s'%(name)] = self.ngram_precision()\n",
    "        output['precision_ngram_freq_%s'%(name)] = self.ngram_precision_freq()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.9 ms\n"
     ]
    }
   ],
   "source": [
    "class spacy_sentences(ny_ingredients):\n",
    "    def __init__(self, fields):\n",
    "        # spacy\n",
    "        self.spacy = spacy.load('en_core_web_lg')\n",
    "        \n",
    "        for field in fields:\n",
    "            assert field in dic[ls[0]].keys()\n",
    "        self.fields = fields # list ['directions']\n",
    "        \n",
    "    def sent(self, listofsent):\n",
    "        doc = self.spacy(' '.join(listofsent))\n",
    "        return [token.lemma_ for token in doc]\n",
    "    def lemma(self):\n",
    "        '''\n",
    "        using the global variables ls and dic\n",
    "        '''\n",
    "        for i, v in dic.items():\n",
    "            if i in ls:\n",
    "                for field in self.fields:\n",
    "                    temp = self.sent(v[field])\n",
    "                    dic[i]['lemma_%s'%(field)]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.53 s\n"
     ]
    }
   ],
   "source": [
    "sp_insr = spacy_sentences(['directions','generated_ingred', 'generated_instr', 'ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "sp_insr.lemma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55102/55102 [00:00<00:00, 78100.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.42 s\n"
     ]
    }
   ],
   "source": [
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        score = metrics(v['ny_ingredients']['exact'], v['lemma_directions'])\n",
    "        dic[i].update(score.all_ngram_recall(name='@baseline_exact'))\n",
    "        score = metrics(v['ny_ingredients']['exact'], v['lemma_generated_ingred'])\n",
    "        dic[i].update(score.all_ngram_recall(name='@test_exact'))\n",
    "        score = metrics(v['ny_ingredients']['exact'], v['lemma_generated_instr'])\n",
    "        dic[i].update(score.all_recall(name='@test2_exact'))\n",
    "        \n",
    "        score = metrics(v['ny_ingredients']['root'], v['lemma_directions'])\n",
    "        dic[i].update(score.all_recall(name='@baseline_root'))\n",
    "        score = metrics(v['ny_ingredients']['root'], v['lemma_generated_ingred'])\n",
    "        dic[i].update(score.all_recall(name='@test_root'))\n",
    "        score = metrics(v['ny_ingredients']['root'], v['lemma_generated_instr'])\n",
    "        dic[i].update(score.all_recall(name='@test2_root'))\n",
    "        \n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "temp = df2[[col for col in df2.columns if '@' in col]].iloc[ls].mean()\n",
    "#logger.report_text(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_ngram_@baseline_exact         0.670009\n",
      "recall_ngram_freq_@baseline_exact    0.671239\n",
      "recall_ngram_@test_exact             0.644259\n",
      "recall_ngram_freq_@test_exact        0.644269\n",
      "recall_@test2_exact                  0.356363\n",
      "recall_freq_@test2_exact             0.357149\n",
      "recall_@baseline_root                0.918265\n",
      "recall_freq_@baseline_root           0.915307\n",
      "recall_@test_root                    0.772434\n",
      "recall_freq_@test_root               0.768093\n",
      "recall_@test2_root                   0.777106\n",
      "recall_freq_@test2_root              0.770032\n",
      "dtype: float64\n",
      "time: 47.8 ms\n"
     ]
    }
   ],
   "source": [
    "# 333k\n",
    "print(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall_ngram_@baseline_exact         0.670009\n",
      "recall_ngram_freq_@baseline_exact    0.671239\n",
      "recall_ngram_@test_exact             0.681311\n",
      "recall_ngram_freq_@test_exact        0.680254\n",
      "recall_@test2_exact                  0.363954\n",
      "recall_freq_@test2_exact             0.362827\n",
      "recall_@baseline_root                0.918265\n",
      "recall_freq_@baseline_root           0.915307\n",
      "recall_@test_root                    0.803410\n",
      "recall_freq_@test_root               0.795026\n",
      "recall_@test2_root                   0.776585\n",
      "recall_freq_@test2_root              0.767976\n",
      "dtype: float64\n",
      "time: 31.4 ms\n"
     ]
    }
   ],
   "source": [
    "# 28k\n",
    "print(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55102/55102 [00:00<00:00, 79626.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "for i, v in tqdm.tqdm(dic.items()):\n",
    "    if i in ls:\n",
    "        score = metrics(v['lemma_directions'], v['ny_generated_ingred']['exact'])\n",
    "        dic[i].update(score.all_ngram_precision(name='#test_exact'))\n",
    "        score = metrics(v['lemma_ingredients'], v['ny_generated_ingred']['exact'])\n",
    "        dic[i].update(score.all_ngram_precision(name='#exact'))\n",
    "        score = metrics(v['lemma_directions'], v['ny_ingredients']['exact'])\n",
    "        dic[i].update(score.all_ngram_precision(name='#baseline_exact'))\n",
    "        \n",
    "        score = metrics(v['lemma_directions'], v['ny_generated_ingred']['root'])\n",
    "        dic[i].update(score.all_ngram_precision(name='#test_root'))\n",
    "        score = metrics(v['lemma_ingredients'], v['ny_generated_ingred']['root'])\n",
    "        dic[i].update(score.all_ngram_precision(name='#root'))\n",
    "        score = metrics(v['lemma_directions'], v['ny_ingredients']['root'])\n",
    "        dic[i].update(score.all_ngram_precision(name='#baseline_root'))\n",
    "\n",
    "df2 = pd.DataFrame.from_dict(dic, orient = 'index')\n",
    "temp = df2[[col for col in df2.columns if '#' in col]].iloc[ls].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_ngram_#test_exact             0.675705\n",
      "precision_ngram_freq_#test_exact        0.608270\n",
      "precision_ngram_#exact                  0.769029\n",
      "precision_ngram_freq_#exact             0.672073\n",
      "precision_ngram_#baseline_exact         0.670009\n",
      "precision_ngram_freq_#baseline_exact    0.671239\n",
      "precision_ngram_#test_root              0.869120\n",
      "precision_ngram_freq_#test_root         0.773899\n",
      "precision_ngram_#root                   0.876402\n",
      "precision_ngram_freq_#root              0.745420\n",
      "precision_ngram_#baseline_root          0.919592\n",
      "precision_ngram_freq_#baseline_root     0.916711\n",
      "dtype: float64\n",
      "time: 42.7 ms\n"
     ]
    }
   ],
   "source": [
    "# 333k\n",
    "print(str(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_ngram_#test_exact             0.682268\n",
      "precision_ngram_freq_#test_exact        0.631267\n",
      "precision_ngram_#exact                  0.778827\n",
      "precision_ngram_freq_#exact             0.703531\n",
      "precision_ngram_#baseline_exact         0.670009\n",
      "precision_ngram_freq_#baseline_exact    0.671239\n",
      "precision_ngram_#test_root              0.877475\n",
      "precision_ngram_freq_#test_root         0.800930\n",
      "precision_ngram_#root                   0.887265\n",
      "precision_ngram_freq_#root              0.781157\n",
      "precision_ngram_#baseline_root          0.919592\n",
      "precision_ngram_freq_#baseline_root     0.916711\n",
      "dtype: float64\n",
      "time: 30.9 ms\n"
     ]
    }
   ],
   "source": [
    "# 28k\n",
    "print(str(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ../../to_gpt2/generation_333k_sorted_truth_t.txt\n",
      "saved ../../to_gpt2/generation_333k_sorted_truth_i.txt\n",
      "saved ../../to_gpt2/generation_333k_sorted_truth_d.txt\n",
      "saved ../../to_gpt2/generation_333k_sorted_pred_t.txt\n",
      "saved ../../to_gpt2/generation_333k_sorted_pred_i.txt\n",
      "saved ../../to_gpt2/generation_333k_sorted_pred_d.txt\n",
      "time: 534 ms\n"
     ]
    }
   ],
   "source": [
    "def add_space(line):\n",
    "    # add space before punct\n",
    "    line = re.sub('([.,!?()])', r' \\1 ', line)\n",
    "    line = re.sub('\\s{2,}', ' ', line)\n",
    "    return line\n",
    "\n",
    "to_write = {'truth_t':'', 'truth_i':'', 'truth_d':'',\n",
    "            'pred_t':'', 'pred_i':'', 'pred_d':''\n",
    "           }\n",
    "for i, v in dic.items():\n",
    "    if i in ls:\n",
    "        to_write['truth_t'] += add_space(' '.join(v['name'])) + '\\n'\n",
    "        to_write['truth_i'] += add_space(' $ '.join(v['ingredients']))+ ' $ \\n'\n",
    "        to_write['truth_d'] += add_space(' '.join(v['directions'])) + ' . \\n'\n",
    "        to_write['pred_t'] += add_space(v['generated_name']) + '\\n'\n",
    "        to_write['pred_i'] += add_space(' $ '.join(v['generated_ingred'])) + ' $ \\n'\n",
    "        to_write['pred_d'] += add_space(' . '.join(v['generated_instr'])) + ' . \\n'\n",
    "        \n",
    "for k, v in to_write.items():\n",
    "    save('../../to_gpt2/generation_333k_sorted_%s.txt'%(k), v ,overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/yueliu/RecipeAnalytics_201906/AA6/NLG_notebooks/tools\n",
      "time: 49 ms\n"
     ]
    }
   ],
   "source": [
    "cd tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 5.80, 34.3/10.7/3.5/0.9 (BP=1.000, ratio=1.002, hyp_len=1988, ref_len=1985)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "!perl multi-bleu.perl ../../../to_gpt2/generation_333k_sorted_truth_t.txt < ../../../to_gpt2/generation_333k_sorted_pred_t.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 27.12, 60.6/37.7/21.6/11.0 (BP=1.000, ratio=1.030, hyp_len=27837, ref_len=27015)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "time: 582 ms\n"
     ]
    }
   ],
   "source": [
    "!perl multi-bleu.perl ../../../to_gpt2/generation_333k_sorted_truth_i.txt < ../../../to_gpt2/generation_333k_sorted_pred_i.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 12.83, 55.1/22.4/9.6/4.6 (BP=0.840, ratio=0.852, hyp_len=51323, ref_len=60267)\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n",
      "time: 955 ms\n"
     ]
    }
   ],
   "source": [
    "!perl multi-bleu.perl ../../../to_gpt2/generation_333k_sorted_truth_d.txt < ../../../to_gpt2/generation_333k_sorted_pred_d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.3494049357953457,\n",
      "    \"p\": 0.34968650793650813,\n",
      "    \"r\": 0.3886880952380953\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.11146750909357371,\n",
      "    \"p\": 0.11326428571428578,\n",
      "    \"r\": 0.12848968253968257\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.3112737859663617,\n",
      "    \"p\": 0.33570476190476206,\n",
      "    \"r\": 0.37238015873015884\n",
      "  }\n",
      "}\n",
      "time: 470 ms\n"
     ]
    }
   ],
   "source": [
    "!rouge -f ../../to_gpt2/generation_28k_sorted/truth_t.txt ../../to_gpt2/generation_28k_sorted/pred_t.txt --avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.658054120038043,\n",
      "    \"p\": 0.6195348040467759,\n",
      "    \"r\": 0.7168047848154568\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.43489691600028596,\n",
      "    \"p\": 0.41772517944310966,\n",
      "    \"r\": 0.46327416881162253\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.36508606093440943,\n",
      "    \"p\": 0.35378023860347446,\n",
      "    \"r\": 0.408473446063344\n",
      "  }\n",
      "}\n",
      "time: 2.21 s\n"
     ]
    }
   ],
   "source": [
    "!rouge -f ../../to_gpt2/generation_28k_sorted/truth_i.txt ../../to_gpt2/generation_28k_sorted/pred_i.txt --avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"f\": 0.5010280412638892,\n",
      "    \"p\": 0.4857465311056858,\n",
      "    \"r\": 0.5544363885970187\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"f\": 0.19623533217129435,\n",
      "    \"p\": 0.19298545738851539,\n",
      "    \"r\": 0.21920300656555444\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"f\": 0.4349484306791758,\n",
      "    \"p\": 0.4443676911244383,\n",
      "    \"r\": 0.5080808218109966\n",
      "  }\n",
      "}\n",
      "time: 8.48 s\n"
     ]
    }
   ],
   "source": [
    "!rouge -f ../../to_gpt2/generation_28k_sorted/truth_d.txt ../../to_gpt2/generation_28k_sorted/pred_d.txt --avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
